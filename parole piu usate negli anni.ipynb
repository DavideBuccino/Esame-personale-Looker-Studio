{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4af580f-7ab8-4214-b2a0-b05cb5fc1951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in d:\\bocci\\anaconda3\\envs\\sanremo-env\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: colorama in d:\\bocci\\anaconda3\\envs\\sanremo-env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d6d9db1-3329-4838-baac-ea4a56e94466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\bocci\\anaconda3\\envs\\sanremo-env\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "427fb0af-4b3a-44b3-a969-df756366baa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bocci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bocci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Tutti i dati sono stati salvati con successo!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Download dei pacchetti necessari di NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Funzione di caricamento dei testi\n",
    "main_folder = \"C:/Users/bocci/OneDrive/Desktop/ESAME FINALE LOOKER STUDIO/Testi brani Sanremo 1951-2025\"\n",
    "\n",
    "def load_texts():\n",
    "    texts = {}\n",
    "    for year in range(1951, 2026):\n",
    "        folder = os.path.join(main_folder, f\"Testi_Sanremo_{year}\")\n",
    "        texts[year] = []\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(folder, filename), 'r', encoding='utf-8') as file:\n",
    "                    texts[year].append(file.read())\n",
    "    return texts\n",
    "\n",
    "texts = load_texts()\n",
    "\n",
    "# Lista di parole comuni da rimuovere (stopwords personalizzate)\n",
    "custom_stopwords = set([\n",
    "    \"son\", \"quando\", \"gi√†\", \"quel\", \"cos√¨\", \"poi\", \"questa\", \"quella\", \"questo\", \"quello\", \"anche\", \"ora\", \n",
    "    \"adesso\", \"√®\", \"la\", \"il\", \"e\", \"di\", \"un\", \"una\", \"per\", \"che\", \"questi\", \"queste\", \"nessuno\", \"alcuni\", \n",
    "    \"alcune\", \"si\", \"c'√®\", \"dove\", \"come\", \"allora\", \"bene\", \"male\", \"tanto\", \"tanto\", \"pi√π\", \"meno\", \"tutti\"\n",
    "])\n",
    "\n",
    "# Funzione di preprocessing dei testi con gestione dell'apostrofo\n",
    "def preprocess_with_apostrophe_handling(text):\n",
    "    # Rimuove i caratteri non alfabetici tranne gli apostrofi\n",
    "    text = re.sub(r'[^a-zA-Z√†√®√©√¨√≤√≥√π√π\\s\\'‚Äô]', '', text.lower())\n",
    "    \n",
    "    # Gestisce le parole con apostrofo, rimuovendo la lettera prima dell'apostrofo\n",
    "    text = re.sub(r\"\\b(\\w)\\'\", '', text)  # Rimuove la lettera prima dell'apostrofo (come \"l'amore\" -> \"amore\")\n",
    "    \n",
    "    # Tokenizzazione e rimozione delle stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('italian')).union(custom_stopwords)\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]  # Evita parole corte\n",
    "    return tokens\n",
    "\n",
    "processed_texts = {year: [preprocess_with_apostrophe_handling(text) for text in texts[year]] for year in texts}\n",
    "\n",
    "# Funzione per calcolare la lunghezza dei testi\n",
    "def text_lengths(year_texts):\n",
    "    return [len(text) for text in year_texts]\n",
    "\n",
    "lengths_per_year = {year: text_lengths(processed_texts[year]) for year in processed_texts}\n",
    "\n",
    "mean_lengths = {year: np.mean(lengths_per_year[year]) for year in lengths_per_year}\n",
    "median_lengths = {year: np.median(lengths_per_year[year]) for year in lengths_per_year}\n",
    "min_lengths = {year: np.min(lengths_per_year[year]) for year in lengths_per_year}\n",
    "max_lengths = {year: np.max(lengths_per_year[year]) for year in lengths_per_year}\n",
    "\n",
    "# Salvataggio dei dati in un file CSV e XLSX\n",
    "lengths_df = pd.DataFrame({\n",
    "    \"Anno\": list(mean_lengths.keys()),\n",
    "    \"Lunghezza Media\": list(mean_lengths.values()),\n",
    "    \"Lunghezza Mediana\": list(median_lengths.values()),\n",
    "    \"Lunghezza Minima\": list(min_lengths.values()),\n",
    "    \"Lunghezza Massima\": list(max_lengths.values())\n",
    "})\n",
    "\n",
    "lengths_df.to_csv(\"sanremo_lunghezze_testi.csv\", index=False)\n",
    "lengths_df.to_excel(\"sanremo_lunghezze_testi.xlsx\", index=False)\n",
    "\n",
    "# Conteggio delle parole pi√π comuni che compaiono in pi√π di un testo\n",
    "def get_most_common_words(year_texts):\n",
    "    # Conta le parole per ogni anno\n",
    "    word_counts = Counter([word for text in year_texts for word in text])\n",
    "    \n",
    "    # Considera solo le parole che appaiono in pi√π di un testo\n",
    "    filtered_words = {word: count for word, count in word_counts.items() if sum(1 for text in year_texts if word in text) > 1}\n",
    "    \n",
    "    # Restituisce le 10 parole pi√π comuni\n",
    "    return Counter(filtered_words).most_common(10)\n",
    "\n",
    "common_words_per_year = {year: get_most_common_words(processed_texts[year]) for year in processed_texts}\n",
    "\n",
    "# Salvataggio delle parole comuni in CSV\n",
    "common_words_data = []\n",
    "for year, common_words in common_words_per_year.items():\n",
    "    for word, freq in common_words:\n",
    "        common_words_data.append([year, word, freq])\n",
    "\n",
    "common_words_df = pd.DataFrame(common_words_data, columns=[\"Anno\", \"Parola\", \"Frequenza\"])\n",
    "common_words_df.to_csv(\"sanremo_parole_comuni.csv\", index=False)\n",
    "common_words_df.to_excel(\"sanremo_parole_comuni.xlsx\", index=False)\n",
    "\n",
    "# Funzione per calcolare le parole pi√π comuni per decennio e venticinquennio\n",
    "def get_most_common_for_periods(period_texts, period_label):\n",
    "    # Conta le parole per il periodo specificato\n",
    "    word_counts = Counter([word for text in period_texts for word in text])\n",
    "    \n",
    "    # Considera solo le parole che appaiono in pi√π di un testo\n",
    "    filtered_words = {word: count for word, count in word_counts.items() if sum(1 for text in period_texts if word in text) > 1}\n",
    "    \n",
    "    # Restituisce le 10 parole pi√π comuni come tuple (period_label, parola, frequenza)\n",
    "    return [(period_label, word, count) for word, count in Counter(filtered_words).most_common(10)]\n",
    "\n",
    "# Raccolta delle parole pi√π comuni per decennio\n",
    "decade_words = []\n",
    "decades = [(1951, 1960), (1961, 1970), (1971, 1980), (1981, 1990), (1991, 2000), (2001, 2010), (2011, 2020), (2021, 2025)]\n",
    "for start, end in decades:\n",
    "    period_texts = [processed_texts[year] for year in range(start, end + 1)]\n",
    "    combined_texts = [text for sublist in period_texts for text in sublist]\n",
    "    decade_words.extend(get_most_common_for_periods(combined_texts, f\"{start}-{end}\"))\n",
    "\n",
    "# Raccolta delle parole pi√π comuni per venticinquennio\n",
    "quarter_words = []\n",
    "quarters = [(1951, 1975), (1976, 2000), (2001, 2025)]\n",
    "for start, end in quarters:\n",
    "    period_texts = [processed_texts[year] for year in range(start, end + 1)]\n",
    "    combined_texts = [text for sublist in period_texts for text in sublist]\n",
    "    quarter_words.extend(get_most_common_for_periods(combined_texts, f\"{start}-{end}\"))\n",
    "\n",
    "# Creazione dei DataFrame per decennio e venticinquennio\n",
    "decade_df = pd.DataFrame(decade_words, columns=[\"Periodo\", \"Parola\", \"Frequenza\"])\n",
    "quarter_df = pd.DataFrame(quarter_words, columns=[\"Periodo\", \"Parola\", \"Frequenza\"])\n",
    "\n",
    "# Salvataggio dei risultati in CSV e XLSX\n",
    "decade_df.to_csv(\"sanremo_parole_comuni_decennio.csv\", index=False)\n",
    "decade_df.to_excel(\"sanremo_parole_comuni_decennio.xlsx\", index=False)\n",
    "\n",
    "quarter_df.to_csv(\"sanremo_parole_comuni_venticinquennio.csv\", index=False)\n",
    "quarter_df.to_excel(\"sanremo_parole_comuni_venticinquennio.xlsx\", index=False)\n",
    "\n",
    "print(\"üìä Tutti i dati sono stati salvati con successo!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "158bc139-a6c0-4d0c-add5-1dde6e656c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bocci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bocci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Elaborazione decenni: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:24<00:00,  3.01s/it]\n",
      "Elaborazione venticinquennio: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:53<00:00, 17.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Tutti i dati sono stati salvati con successo!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from tqdm import tqdm  # Per il monitoraggio dell'avanzamento\n",
    "\n",
    "# Download dei pacchetti necessari di NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Funzione di caricamento dei testi\n",
    "main_folder = \"C:/Users/bocci/OneDrive/Desktop/ESAME FINALE LOOKER STUDIO/Testi brani Sanremo 1951-2025\"\n",
    "\n",
    "def load_texts():\n",
    "    texts = {}\n",
    "    for year in range(1951, 2026):\n",
    "        folder = os.path.join(main_folder, f\"Testi_Sanremo_{year}\")\n",
    "        texts[year] = []\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(folder, filename), 'r', encoding='utf-8') as file:\n",
    "                    texts[year].append(file.read())\n",
    "    return texts\n",
    "\n",
    "texts = load_texts()\n",
    "\n",
    "# Parole chiave per le categorie (anatomia, natura, tecnologia)\n",
    "anatomy_keywords = set([ \n",
    "    \"cuore\", \"polmoni\", \"stomaco\", \"testa\", \"mani\", \"piedi\", \"occhi\", \"braccia\", \"gambe\", \"schiena\", \"pelle\", \n",
    "    \"dita\", \"bocca\", \"denti\", \"orecchie\", \"cervello\", \"nervi\", \"vene\", \"arterie\", \"organi\", \"corpo\", \"ossa\", \n",
    "    \"sangue\", \"respirare\", \"battere\", \"muovere\", \"camminare\", \"correre\", \"pensare\", \"vedere\", \"sentire\", \"toccare\", \n",
    "    \"gustare\", \"ascoltare\", \"dormire\", \"mangiare\", \"parlare\", \"piangere\"\n",
    "])\n",
    "\n",
    "nature_keywords = set([\n",
    "    \"sole\", \"luna\", \"cielo\", \"stella\", \"nuvole\", \"mare\", \"fiume\", \"montagna\", \"collina\", \"valle\", \"foresta\", \n",
    "    \"alberi\", \"erba\", \"sabbia\", \"terra\", \"vento\", \"pioggia\", \"neve\", \"luce\", \"ombra\", \"ghiaccio\", \"fiori\", \"piante\", \n",
    "    \"uccelli\", \"pesci\", \"cani\", \"gatti\", \"cavalli\", \"api\", \"farfalle\", \"insetti\", \"bosco\", \"fauna\", \"flora\", \"natura\"\n",
    "])\n",
    "\n",
    "tech_keywords = set([\n",
    "    \"internet\", \"computer\", \"rete\", \"software\", \"hardware\", \"digitale\", \"social\", \"smartphone\", \"applicazione\", \n",
    "    \"programma\", \"sito\", \"cloud\", \"server\", \"codice\", \"algoritmo\", \"byte\", \"dati\", \"pixel\", \"schermo\", \"monitor\", \n",
    "    \"robot\", \"automazione\", \"intelligenza artificiale\", \"vr\", \"ar\", \"droni\", \"macchina\", \"internet delle cose\", \"iot\", \n",
    "    \"smart\", \"instagram\", \"tik tok\", \"reel\", \"reels\", \"storie\", \"foto\", \"video\"\n",
    "])\n",
    "\n",
    "# Lista di parole comuni da rimuovere (stopwords generali)\n",
    "custom_stopwords = set([\n",
    "    \"son\", \"quando\", \"gi√†\", \"quel\", \"cos√¨\", \"poi\", \"questa\", \"quella\", \"questo\", \"quello\", \"anche\", \"ora\", \n",
    "    \"adesso\", \"√®\", \"la\", \"il\", \"e\", \"di\", \"un\", \"una\", \"per\", \"che\", \"questi\", \"queste\", \"nessuno\", \"alcuni\", \n",
    "    \"alcune\", \"si\", \"c'√®\", \"dove\", \"come\", \"allora\", \"bene\", \"male\", \"tanto\", \"pi√π\", \"meno\", \"tutti\"\n",
    "])\n",
    "\n",
    "# Funzione di preprocessing dei testi con gestione dell'apostrofo\n",
    "def preprocess_with_apostrophe_handling(text):\n",
    "    # Rimuove i caratteri non alfabetici tranne gli apostrofi\n",
    "    text = re.sub(r'[^a-zA-Z√†√®√©√¨√≤√≥√π√π\\s\\'‚Äô]', '', text.lower())\n",
    "    \n",
    "    # Gestisce le parole con apostrofo, rimuovendo la lettera prima dell'apostrofo\n",
    "    text = re.sub(r\"\\b(\\w)\\'\", '', text)  # Rimuove la lettera prima dell'apostrofo (come \"l'amore\" -> \"amore\")\n",
    "    \n",
    "    # Tokenizzazione e rimozione delle stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('italian')).union(custom_stopwords)\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]  # Evita parole corte\n",
    "    return tokens\n",
    "\n",
    "processed_texts = {year: [preprocess_with_apostrophe_handling(text) for text in texts[year]] for year in texts}\n",
    "\n",
    "# Funzione per calcolare le parole pi√π comuni, considerando solo quelle presenti in pi√π di una canzone\n",
    "def get_most_common_words(year_texts):\n",
    "    word_counts = Counter([word for text in year_texts for word in text])\n",
    "    filtered_words = {word: count for word, count in word_counts.items() if sum(1 for text in year_texts if word in text) > 1}\n",
    "    return Counter(filtered_words).most_common(10)\n",
    "\n",
    "common_words_per_year = {year: get_most_common_words(processed_texts[year]) for year in processed_texts}\n",
    "\n",
    "# Funzione per estrarre le parole chiave nelle categorie anatomia, natura e tecnologia\n",
    "def extract_keywords_from_text(texts, keywords_set):\n",
    "    flat_texts = [word for text in texts for word in text]  # Appiattisce la lista di testi\n",
    "    return [word for word in flat_texts if word in keywords_set]\n",
    "\n",
    "# Estrazione delle parole chiave per anatomia, natura e tecnologia\n",
    "all_texts = [text for sublist in processed_texts.values() for text in sublist]\n",
    "\n",
    "anatomy_words = extract_keywords_from_text(all_texts, anatomy_keywords)\n",
    "nature_words = extract_keywords_from_text(all_texts, nature_keywords)\n",
    "tech_words = extract_keywords_from_text(all_texts, tech_keywords)\n",
    "\n",
    "# Funzione per calcolare le parole pi√π comuni per decennio e venticinquennio\n",
    "def get_most_common_for_periods(period_texts, period_label):\n",
    "    word_counts = Counter([word for text in period_texts for word in text])\n",
    "    filtered_words = {word: count for word, count in word_counts.items() if sum(1 for text in period_texts if word in text) > 1}\n",
    "    return [(period_label, word, count) for word, count in Counter(filtered_words).most_common(10)]\n",
    "\n",
    "# Raccolta delle parole comuni per decennio e venticinquennio\n",
    "decade_words = []\n",
    "decades = [(1951, 1960), (1961, 1970), (1971, 1980), (1981, 1990), (1991, 2000), (2001, 2010), (2011, 2020), (2021, 2025)]\n",
    "\n",
    "for start, end in tqdm(decades, desc=\"Elaborazione decenni\"):\n",
    "    period_texts = [processed_texts[year] for year in range(start, end + 1)]\n",
    "    combined_texts = [text for sublist in period_texts for text in sublist]\n",
    "    decade_words.extend(get_most_common_for_periods(combined_texts, f\"{start}-{end}\"))\n",
    "\n",
    "quarter_words = []\n",
    "quarters = [(1951, 1975), (1976, 2000), (2001, 2025)]\n",
    "\n",
    "for start, end in tqdm(quarters, desc=\"Elaborazione venticinquennio\"):\n",
    "    period_texts = [processed_texts[year] for year in range(start, end + 1)]\n",
    "    combined_texts = [text for sublist in period_texts for text in sublist]\n",
    "    quarter_words.extend(get_most_common_for_periods(combined_texts, f\"{start}-{end}\"))\n",
    "\n",
    "# Creazione dei DataFrame per decennio e venticinquennio\n",
    "decade_df = pd.DataFrame(decade_words, columns=[\"Periodo\", \"Parola\", \"Frequenza\"])\n",
    "quarter_df = pd.DataFrame(quarter_words, columns=[\"Periodo\", \"Parola\", \"Frequenza\"])\n",
    "\n",
    "# Creazione dei DataFrame per le categorie\n",
    "anatomy_df = pd.DataFrame(Counter(anatomy_words).items(), columns=[\"Parola\", \"Frequenza\"])\n",
    "nature_df = pd.DataFrame(Counter(nature_words).items(), columns=[\"Parola\", \"Frequenza\"])\n",
    "tech_df = pd.DataFrame(Counter(tech_words).items(), columns=[\"Parola\", \"Frequenza\"])\n",
    "\n",
    "# Creazione di un unico file Excel con 3 fogli\n",
    "with pd.ExcelWriter(\"sanremo_parole_per_categoria.xlsx\") as writer:\n",
    "    anatomy_df.to_excel(writer, sheet_name=\"Anatomia\", index=False)\n",
    "    nature_df.to_excel(writer, sheet_name=\"Natura\", index=False)\n",
    "    tech_df.to_excel(writer, sheet_name=\"Tecnologia\", index=False)\n",
    "\n",
    "# Creazione dei file per il conteggio delle parole comuni\n",
    "common_words_data = []\n",
    "for year, common_words in common_words_per_year.items():\n",
    "    for word, freq in common_words:\n",
    "        common_words_data.append([year, word, freq])\n",
    "\n",
    "common_words_df = pd.DataFrame(common_words_data, columns=[\"Anno\", \"Parola\", \"Frequenza\"])\n",
    "\n",
    "# Salvataggio delle parole comuni in un file Excel\n",
    "with pd.ExcelWriter(\"sanremo_parole_comuni.xlsx\") as writer:\n",
    "    common_words_df.to_excel(writer, index=False)\n",
    "\n",
    "# Salvataggio dei dati per decennio e venticinquennio\n",
    "with pd.ExcelWriter(\"sanremo_parole_comuni_decennio_e_venticinquennio.xlsx\") as writer:\n",
    "    decade_df.to_excel(writer, sheet_name=\"Decenni\", index=False)\n",
    "    quarter_df.to_excel(writer, sheet_name=\"Venticinquennio\", index=False)\n",
    "\n",
    "print(\"üìä Tutti i dati sono stati salvati con successo!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9118067-09d5-4841-b097-e544a1989de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bocci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bocci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Tutti i dati sono stati salvati con successo!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Download dei pacchetti necessari di NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Funzione di caricamento dei testi\n",
    "main_folder = \"C:/Users/bocci/OneDrive/Desktop/ESAME FINALE LOOKER STUDIO/Testi brani Sanremo 1951-2025\"\n",
    "\n",
    "def load_texts():\n",
    "    texts = {}\n",
    "    for year in range(1951, 2026):\n",
    "        folder = os.path.join(main_folder, f\"Testi_Sanremo_{year}\")\n",
    "        texts[year] = []\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(folder, filename), 'r', encoding='utf-8') as file:\n",
    "                    texts[year].append(file.read())\n",
    "    return texts\n",
    "\n",
    "texts = load_texts()\n",
    "\n",
    "# Lista di parole comuni da rimuovere (stopwords personalizzate)\n",
    "custom_stopwords = set([\n",
    "    \"son\", \"quando\", \"gi√†\", \"quel\", \"cos√¨\", \"poi\", \"questa\", \"quella\", \"questo\", \"quello\", \"anche\", \"ora\", \n",
    "    \"adesso\", \"√®\", \"la\", \"il\", \"e\", \"di\", \"un\", \"una\", \"per\", \"che\", \"questi\", \"queste\", \"nessuno\", \"alcuni\", \n",
    "    \"alcune\", \"si\", \"c'√®\", \"dove\", \"come\", \"allora\", \"bene\", \"male\", \"tanto\", \"tanto\", \"pi√π\", \"meno\", \"tutti\"\n",
    "])\n",
    "\n",
    "# Funzione di preprocessing dei testi con gestione dell'apostrofo\n",
    "def preprocess_with_apostrophe_handling(text):\n",
    "    # Rimuove i caratteri non alfabetici tranne gli apostrofi\n",
    "    text = re.sub(r'[^a-zA-Z√†√®√©√¨√≤√≥√π√π\\s\\'‚Äô]', '', text.lower())\n",
    "    \n",
    "    # Gestisce le parole con apostrofo, rimuovendo la lettera prima dell'apostrofo\n",
    "    text = re.sub(r\"\\b(\\w)\\'\", '', text)  # Rimuove la lettera prima dell'apostrofo (come \"l'amore\" -> \"amore\")\n",
    "    \n",
    "    # Tokenizzazione e rimozione delle stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('italian')).union(custom_stopwords)\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]  # Evita parole corte\n",
    "    return tokens\n",
    "\n",
    "processed_texts = {year: [preprocess_with_apostrophe_handling(text) for text in texts[year]] for year in texts}\n",
    "\n",
    "# Funzione per calcolare la lunghezza dei testi\n",
    "def text_lengths(year_texts):\n",
    "    return [len(text) for text in year_texts]\n",
    "\n",
    "lengths_per_year = {year: text_lengths(processed_texts[year]) for year in processed_texts}\n",
    "\n",
    "mean_lengths = {year: np.mean(lengths_per_year[year]) for year in lengths_per_year}\n",
    "median_lengths = {year: np.median(lengths_per_year[year]) for year in lengths_per_year}\n",
    "min_lengths = {year: np.min(lengths_per_year[year]) for year in lengths_per_year}\n",
    "max_lengths = {year: np.max(lengths_per_year[year]) for year in lengths_per_year}\n",
    "\n",
    "# Creazione del DataFrame per la lunghezza dei testi\n",
    "lengths_df = pd.DataFrame({\n",
    "    \"Anno\": list(mean_lengths.keys()),\n",
    "    \"Lunghezza Media\": list(mean_lengths.values()),\n",
    "    \"Lunghezza Mediana\": list(median_lengths.values()),\n",
    "    \"Lunghezza Minima\": list(min_lengths.values()),\n",
    "    \"Lunghezza Massima\": list(max_lengths.values())\n",
    "})\n",
    "\n",
    "lengths_df.to_excel(\"sanremo_lunghezze_testi.xlsx\", index=False)\n",
    "\n",
    "# Lista di categorie di parole (anatomia, natura, tecnologia)\n",
    "categories = {\n",
    "    \"anatomia\": [\"cuore\", \"testa\", \"braccio\", \"mano\", \"occhio\", \"bocca\", \"gamba\", \"schiena\", \"stomaco\", \"collo\"],\n",
    "    \"natura\": [\"albero\", \"fiore\", \"acqua\", \"mare\", \"montagna\", \"cielo\", \"sole\", \"terra\", \"luna\", \"stella\"],\n",
    "    \"tecnologia\": [\"computer\", \"internet\", \"smartphone\", \"tecnologia\", \"robot\", \"digitale\", \"software\", \"hardware\", \"rete\", \"app\"]\n",
    "}\n",
    "\n",
    "# Funzione per calcolare le parole pi√π comuni che compaiono in pi√π di una canzone per categoria\n",
    "def get_most_common_words_for_category(year_texts, category):\n",
    "    word_counts = Counter([word for text in year_texts for word in text if word in category])\n",
    "    \n",
    "    # Considera solo le parole che appaiono in pi√π di un testo\n",
    "    filtered_words = {word: count for word, count in word_counts.items() if sum(1 for text in year_texts if word in text) > 1}\n",
    "    \n",
    "    return Counter(filtered_words).most_common(10)\n",
    "\n",
    "# Funzione per ottenere parole comuni per periodi (decennio, venticinquennio)\n",
    "def get_most_common_for_periods(period_texts, period_label, category):\n",
    "    word_counts = Counter([word for text in period_texts for word in text if word in category])\n",
    "    \n",
    "    # Considera solo le parole che appaiono in pi√π di un testo\n",
    "    filtered_words = {word: count for word, count in word_counts.items() if sum(1 for text in period_texts if word in text) > 1}\n",
    "    \n",
    "    return [(period_label, word, count) for word, count in Counter(filtered_words).most_common(10)]\n",
    "\n",
    "# Funzione per raccogliere parole comuni per categoria su tutti gli anni\n",
    "def collect_category_words_for_all_years():\n",
    "    category_words_data = []\n",
    "    for category_name, category_words in categories.items():\n",
    "        for year, year_texts in processed_texts.items():\n",
    "            common_words = get_most_common_words_for_category(year_texts, category_words)\n",
    "            for word, freq in common_words:\n",
    "                category_words_data.append([year, category_name, word, freq])\n",
    "    return category_words_data\n",
    "\n",
    "# Raccolta delle parole comuni per categoria\n",
    "category_words_data = collect_category_words_for_all_years()\n",
    "\n",
    "# Creazione del DataFrame e salvataggio in Excel\n",
    "category_words_df = pd.DataFrame(category_words_data, columns=[\"Anno\", \"Categoria\", \"Parola\", \"Frequenza\"])\n",
    "category_words_df.to_excel(\"sanremo_parole_comuni_per_categoria.xlsx\", index=False)\n",
    "\n",
    "print(\"üìä Tutti i dati sono stati salvati con successo!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e94bbef1-b2ca-48fa-a095-4034ee605d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bocci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bocci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Download dei pacchetti necessari di NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Funzione di caricamento dei testi\n",
    "main_folder = \"C:/Users/bocci/OneDrive/Desktop/ESAME FINALE LOOKER STUDIO/Testi brani Sanremo 1951-2025\"\n",
    "\n",
    "def load_texts():\n",
    "    texts = {}\n",
    "    for year in range(1951, 2026):\n",
    "        folder = os.path.join(main_folder, f\"Testi_Sanremo_{year}\")\n",
    "        texts[year] = []\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(folder, filename), 'r', encoding='utf-8') as file:\n",
    "                    texts[year].append(file.read())\n",
    "    return texts\n",
    "\n",
    "texts = load_texts()\n",
    "\n",
    "# Liste di parole comuni da rimuovere (stopwords personalizzate)\n",
    "anatomy_keywords = set([\n",
    "    \"cuore\", \"polmoni\", \"stomaco\", \"testa\", \"mani\", \"piedi\", \"occhi\", \"braccia\", \"gambe\", \"schiena\", \"pelle\", \"dita\", \"bocca\", \"denti\", \"orecchie\", \"cervello\", \"nervi\", \"vene\", \"arterie\", \"organi\", \"corpo\", \"ossa\", \"sangue\",\n",
    "    \"respirare\", \"battere\", \"muovere\", \"camminare\", \"correre\", \"pensare\", \"vedere\", \"sentire\", \"toccare\", \"gustare\", \"ascoltare\", \"dormire\", \"mangiare\", \"parlare\", \"piangere\"\n",
    "])\n",
    "\n",
    "nature_keywords = set([\n",
    "    \"sole\", \"luna\", \"cielo\", \"stella\", \"nuvole\", \"mare\", \"fiume\", \"montagna\", \"collina\", \"valle\", \"foresta\", \"alberi\", \"erba\", \"sabbia\", \"terra\", \"vento\", \"pioggia\", \"neve\", \"luce\", \"ombra\", \"ghiaccio\",\n",
    "    \"fiori\", \"piante\", \"uccelli\", \"pesci\", \"cani\", \"gatti\", \"cavalli\", \"api\", \"farfalle\", \"insetti\", \"bosco\", \"fauna\", \"flora\", \"natura\"\n",
    "])\n",
    "\n",
    "tech_keywords = set([\n",
    "    \"internet\", \"computer\", \"rete\", \"software\", \"hardware\", \"digitale\", \"social\", \"smartphone\", \"applicazione\", \"programma\", \"sito\", \"cloud\", \"server\", \"codice\", \"algoritmo\", \"byte\", \"dati\", \"pixel\", \"schermo\", \"monitor\",\n",
    "    \"robot\", \"automazione\", \"intelligenza artificiale\", \"vr\", \"ar\", \"droni\", \"macchina\", \"internet delle cose\", \"iot\", \"smart\",\n",
    "    \"instagram\", \"tiktok\", \"reels\", \"storiedistagram\", \"foto\", \"video\", \"post\", \"like\", \"followers\", \"hashtag\", \"socialnetwork\"\n",
    "])\n",
    "\n",
    "# Lista di parole comuni da rimuovere (stopwords generali)\n",
    "custom_stopwords = set([\n",
    "    \"son\", \"quando\", \"gi√†\", \"quel\", \"cos√¨\", \"poi\", \"questa\", \"quella\", \"questo\", \"quello\", \"anche\", \"ora\", \n",
    "    \"adesso\", \"√®\", \"la\", \"il\", \"e\", \"di\", \"un\", \"una\", \"per\", \"che\", \"questi\", \"queste\", \"nessuno\", \"alcuni\", \n",
    "    \"alcune\", \"si\", \"c'√®\", \"dove\", \"come\", \"allora\", \"bene\", \"male\", \"tanto\", \"pi√π\", \"meno\", \"tutti\"\n",
    "])\n",
    "\n",
    "# Funzione di preprocessing dei testi con gestione dell'apostrofo\n",
    "def preprocess_with_apostrophe_handling(text):\n",
    "    # Rimuove i caratteri non alfabetici tranne gli apostrofi\n",
    "    text = re.sub(r'[^a-zA-Z√†√®√©√¨√≤√≥√π√π\\s\\'‚Äô]', '', text.lower())\n",
    "    \n",
    "    # Gestisce le parole con apostrofo, rimuovendo la lettera prima dell'apostrofo\n",
    "    text = re.sub(r\"\\b(\\w)\\'\", '', text)  # Rimuove la lettera prima dell'apostrofo (come \"l'amore\" -> \"amore\")\n",
    "    \n",
    "    # Tokenizzazione e rimozione delle stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('italian')).union(custom_stopwords)\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]  # Evita parole corte\n",
    "    return tokens\n",
    "\n",
    "processed_texts = {year: [preprocess_with_apostrophe_handling(text) for text in texts[year]] for year in texts}\n",
    "\n",
    "# Funzione per contare le occorrenze di parole per categoria\n",
    "def count_category_keywords(texts, category_keywords):\n",
    "    counts = Counter()\n",
    "    for text in texts:\n",
    "        counts.update([word for word in text if word in category_keywords])\n",
    "    return counts\n",
    "\n",
    "# Funzione per ottenere le parole pi√π comuni per periodo\n",
    "def get_most_common_for_periods(period_texts, period_label):\n",
    "    word_counts = Counter([word for text in period_texts for word in text])\n",
    "    filtered_words = {word: count for word, count in word_counts.items() if sum(1 for text in period_texts if word in text) > 1}\n",
    "    \n",
    "    # Usa Counter per ordinare e ottenere le parole pi√π comuni\n",
    "    filtered_counter = Counter(filtered_words)\n",
    "    return [(period_label, word, count) for word, count in filtered_counter.most_common(10)]\n",
    "\n",
    "# Raccolta delle parole pi√π comuni per decennio\n",
    "decade_words = []\n",
    "for start, end in [(1951, 1960), (1961, 1970), (1971, 1980), (1981, 1990), (1991, 2000), (2001, 2010), (2011, 2020), (2021, 2025)]:\n",
    "    period_texts = [processed_texts[year] for year in range(start, end + 1)]\n",
    "    combined_texts = [text for sublist in period_texts for text in sublist]\n",
    "    decade_words.extend(get_most_common_for_periods(combined_texts, f\"{start}-{end}\"))\n",
    "\n",
    "# Raccolta delle parole pi√π comuni per venticinquennio\n",
    "quarter_words = []\n",
    "for start, end in [(1951, 1975), (1976, 2000), (2001, 2025)]:\n",
    "    period_texts = [processed_texts[year] for year in range(start, end + 1)]\n",
    "    combined_texts = [text for sublist in period_texts for text in sublist]\n",
    "    quarter_words.extend(get_most_common_for_periods(combined_texts, f\"{start}-{end}\"))\n",
    "\n",
    "# Calcolo delle occorrenze per le categorie anatomiche, naturali e tecnologiche\n",
    "anatomy_counts_per_year = {year: count_category_keywords(processed_texts[year], anatomy_keywords) for year in processed_texts}\n",
    "nature_counts_per_year = {year: count_category_keywords(processed_texts[year], nature_keywords) for year in processed_texts}\n",
    "tech_counts_per_year = {year: count_category_keywords(processed_texts[year], tech_keywords) for year in processed_texts}\n",
    "\n",
    "# Aggiungi i conteggi nel DataFrame per decennio e venticinquennio\n",
    "def add_category_counts(df, period, category_counts):\n",
    "    for category, counts in category_counts.items():\n",
    "        df.loc[df['Periodo'] == period, category] = sum(counts.values())\n",
    "\n",
    "# Creazione del DataFrame per decennio e venticinquennio\n",
    "decade_df = pd.DataFrame(decade_words, columns=[\"Periodo\", \"Parola\", \"Frequenza\"])\n",
    "quarter_df = pd.DataFrame(quarter_words, columns=[\"Periodo\", \"Parola\", \"Frequenza\"])\n",
    "\n",
    "# Aggiungi i conteggi delle categorie per decennio\n",
    "for start, end in [(1951, 1960), (1961, 1970), (1971, 1980), (1981, 1990), (1991, 2000), (2001, 2010), (2011, 2020), (2021, 2025)]:\n",
    "    period = f\"{start}-{end}\"\n",
    "    anatomy_counts = {year: anatomy_counts_per_year[year] for year in range(start, end + 1)}\n",
    "    nature_counts = {year: nature_counts_per_year[year] for year in range(start, end + 1)}\n",
    "    tech_counts = {year: tech_counts_per_year[year] for year in range(start, end + 1)}\n",
    "    \n",
    "    add_category_counts(decade_df, period, anatomy_counts)\n",
    "    add_category_counts(decade_df, period, nature_counts)\n",
    "    add_category_counts(decade_df, period, tech_counts)\n",
    "\n",
    "# Aggiungi i conteggi delle categorie per venticinquennio\n",
    "for start, end in [(1951, 1975), (1976, 2000), (2001, 2025)]:\n",
    "    period = f\"{start}-{end}\"\n",
    "    anatomy_counts = {year: anatomy_counts_per_year[year] for year in range(start, end + 1)}\n",
    "    nature_counts = {year: nature_counts_per_year[year] for year in range(start, end + 1)}\n",
    "    tech_counts = {year: tech_counts_per_year[year] for year in range(start, end + 1)}\n",
    "    \n",
    "    add_category_counts(quarter_df, period, anatomy_counts)\n",
    "    add_category_counts(quarter_df, period, nature_counts)\n",
    "    add_category_counts(quarter_df, period, tech_counts)\n",
    "\n",
    "# Salvataggio dei risultati in XLSX\n",
    "with pd.ExcelWriter(\"sanremo_parole_comuni_decennio.xlsx\") as writer:\n",
    "    # Creazione di fogli separati per ogni categoria\n",
    "    anatomy_df = decade_df[['Periodo', 'Parola', 'Frequenza']]\n",
    "    anatomy_df = anatomy_df[anatomy_df['Parola'].isin(anatomy_keywords)]\n",
    "    anatomy_df.to_excel(writer, sheet_name=\"Anatomia\", index=False)\n",
    "\n",
    "    nature_df = decade_df[['Periodo', 'Parola', 'Frequenza']]\n",
    "    nature_df = nature_df[nature_df['Parola'].isin(nature_keywords)]\n",
    "    nature_df.to_excel(writer, sheet_name=\"Natura\", index=False)\n",
    "\n",
    "    tech_df = decade_df[['Periodo', 'Parola', 'Frequenza']]\n",
    "    tech_df = tech_df[tech_df['Parola'].isin(tech_keywords)]\n",
    "    tech_df.to_excel(writer, sheet_name=\"Tecnologia\", index=False)\n",
    "\n",
    "with pd.ExcelWriter(\"sanremo_parole_comuni_quarter.xlsx\") as writer:\n",
    "    # Creazione di fogli separati per ogni categoria\n",
    "    anatomy_df = quarter_df[['Periodo', 'Parola', 'Frequenza']]\n",
    "    anatomy_df = anatomy_df[anatomy_df['Parola'].isin(anatomy_keywords)]\n",
    "    anatomy_df.to_excel(writer, sheet_name=\"Anatomia\", index=False)\n",
    "\n",
    "    nature_df = quarter_df[['Periodo', 'Parola', 'Frequenza']]\n",
    "    nature_df = nature_df[nature_df['Parola'].isin(nature_keywords)]\n",
    "    nature_df.to_excel(writer, sheet_name=\"Natura\", index=False)\n",
    "\n",
    "    tech_df = quarter_df[['Periodo', 'Parola', 'Frequenza']]\n",
    "    tech_df = tech_df[tech_df['Parola'].isin(tech_keywords)]\n",
    "    tech_df.to_excel(writer, sheet_name=\"Tecnologia\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d0437d-0396-4210-91e8-d9440416b1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Nome Ambiente Secondario",
   "language": "python",
   "name": "nome_ambiente_secondario"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
