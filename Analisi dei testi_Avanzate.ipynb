{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeb7de38-7b44-4d02-b86d-a15acbc24a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Analisi completata. File salvato in: F:\\EPICODE\\DESKTOP PUGLIA\\ESAME FINALE LOOKER STUDIO\\Testi_brani_Sanremo_1951-2025\\analisi_lessico_sanremo.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# Percorso principale dei testi\n",
    "base_path = r\"F:\\EPICODE\\DESKTOP PUGLIA\\ESAME FINALE LOOKER STUDIO\\Testi_brani_Sanremo_1951-2025\"\n",
    "\n",
    "# Funzione di pulizia e tokenizzazione\n",
    "def clean_and_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "# Inizializzazione strutture dati\n",
    "song_data = []\n",
    "\n",
    "# Itera su tutte le cartelle anno\n",
    "for year_folder in os.listdir(base_path):\n",
    "    if year_folder.startswith(\"Testi_Sanremo_\"):\n",
    "        year = int(year_folder.split(\"_\")[-1])\n",
    "        year_path = os.path.join(base_path, year_folder)\n",
    "\n",
    "        for file in os.listdir(year_path):\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(year_path, file)\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        text = f.read()\n",
    "                except UnicodeDecodeError:\n",
    "                    with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
    "                        text = f.read()\n",
    "                \n",
    "                words = clean_and_tokenize(text)\n",
    "                word_count = len(words)\n",
    "                unique_word_count = len(set(words))\n",
    "                lexical_diversity = unique_word_count / word_count if word_count else 0\n",
    "\n",
    "                song_data.append({\n",
    "                    \"anno\": year,\n",
    "                    \"titolo\": file.replace(\".txt\", \"\"),\n",
    "                    \"parole_totali\": word_count,\n",
    "                    \"parole_uniche\": unique_word_count,\n",
    "                    \"diversita_lessicale\": round(lexical_diversity, 3)\n",
    "                })\n",
    "\n",
    "# Creazione DataFrame\n",
    "df = pd.DataFrame(song_data)\n",
    "\n",
    "# Salvataggio XLSX per Looker Studio\n",
    "output_xlsx = os.path.join(base_path, \"analisi_lessico_sanremo.xlsx\")\n",
    "df.to_excel(output_xlsx, index=False)\n",
    "\n",
    "\n",
    "print(\"✅ Analisi completata. File salvato in:\", output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b484d37c-547a-46a6-a1bd-1fc2d424db5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File salvato in: F:\\EPICODE\\DESKTOP PUGLIA\\ESAME FINALE LOOKER STUDIO\\Testi_brani_Sanremo_1951-2025\\Analisi_Semantica_Sanremo.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Percorso base dei testi\n",
    "base_path = r\"F:\\EPICODE\\DESKTOP PUGLIA\\ESAME FINALE LOOKER STUDIO\\Testi_brani_Sanremo_1951-2025\"\n",
    "\n",
    "# Funzione per pulire e tokenizzare il testo\n",
    "def clean_and_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "# Dizionario semantico\n",
    "semantic_categories = {\n",
    "    \"amore\": [\n",
    "        \"amore\", \"amori\", \"amato\", \"amata\", \"amati\", \"amate\", \"amare\", \"amando\", \"amante\", \"amanti\",\n",
    "        \"cuore\", \"cuori\", \"bacio\", \"baci\", \"abbraccio\", \"abbracci\", \"passione\", \"passioni\",\n",
    "        \"innamorato\", \"innamorata\", \"innamorati\", \"innamorate\", \"desiderio\", \"desideri\",\n",
    "        \"carezza\", \"carezze\", \"affetto\", \"affetti\", \"tenerezza\", \"tenerezze\"\n",
    "    ],\n",
    "    \"tristezza\": [\n",
    "        \"pianto\", \"pianti\", \"lacrima\", \"lacrime\", \"triste\", \"tristi\", \"tristezza\", \"tristezze\",\n",
    "        \"solitudine\", \"solitudini\", \"dolore\", \"dolori\", \"perdere\", \"perso\", \"persa\", \"persi\", \"perse\",\n",
    "        \"addio\", \"addii\", \"vuoto\", \"vuoti\", \"rimpianto\", \"rimpianti\", \"assenza\", \"assenze\",\n",
    "        \"nostalgia\", \"nostalgie\", \"disperazione\", \"disperazioni\"\n",
    "    ],\n",
    "    \"gioia\": [\n",
    "        \"gioia\", \"gioie\", \"felice\", \"felici\", \"felicità\", \"riso\", \"risa\", \"ridere\", \"sorriso\", \"sorrisi\",\n",
    "        \"ballare\", \"ballo\", \"balli\", \"festa\", \"feste\", \"speranza\", \"speranze\", \"abbraccio\", \"abbracci\",\n",
    "        \"vivere\", \"vita\", \"vite\", \"amico\", \"amica\", \"amici\", \"amiche\", \"allegria\", \"allegrie\",\n",
    "        \"entusiasmo\", \"entusiasmi\", \"euforia\", \"euforie\"\n",
    "    ],\n",
    "    \"guerra\": [\n",
    "        \"battaglia\", \"battaglie\", \"guerra\", \"guerre\", \"arma\", \"armi\", \"combattere\", \"combattuto\",\n",
    "        \"combattuta\", \"combattuti\", \"combattute\", \"morte\", \"morti\", \"morto\", \"morta\", \"odio\", \"odii\",\n",
    "        \"sangue\", \"sangui\", \"violenza\", \"violenze\", \"soldato\", \"soldati\", \"nemico\", \"nemica\", \"nemici\", \"nemiche\"\n",
    "    ],\n",
    "    \"natura\": [\n",
    "        \"sole\", \"soli\", \"mare\", \"mari\", \"vento\", \"venti\", \"cielo\", \"cieli\", \"terra\", \"terre\", \"fiore\", \"fiori\",\n",
    "        \"notte\", \"notti\", \"alba\", \"albe\", \"pioggia\", \"piogge\", \"neve\", \"nevi\", \"montagna\", \"montagne\",\n",
    "        \"bosco\", \"boschi\", \"stella\", \"stelle\", \"luna\", \"lune\"\n",
    "    ],\n",
    "    \"tempo\": [\n",
    "        \"tempo\", \"tempi\", \"ieri\", \"oggi\", \"domani\", \"attimo\", \"attimi\", \"eterno\", \"eterna\", \"eterni\", \"eterne\",\n",
    "        \"momento\", \"momenti\", \"secondo\", \"secondi\", \"giorno\", \"giorni\", \"sera\", \"sere\", \"notte\", \"notti\",\n",
    "        \"tramonto\", \"tramonti\", \"alba\", \"albe\"\n",
    "    ],\n",
    "    \"identità\": [\n",
    "        \"io\", \"me\", \"persona\", \"persone\", \"anima\", \"anime\", \"nome\", \"nomi\", \"esisto\", \"esistere\", \"vero\", \"vera\",\n",
    "        \"veri\", \"vere\", \"maschera\", \"maschere\", \"cuore\", \"cuori\", \"sogno\", \"sogni\", \"identità\",\n",
    "        \"sé\", \"se stesso\", \"se stessa\", \"sé stessi\", \"sé stesse\"\n",
    "    ],\n",
    "    \"famiglia\": [\n",
    "        \"madre\", \"madri\", \"padre\", \"padri\", \"figlio\", \"figlia\", \"figli\", \"figlie\", \"fratello\", \"fratelli\",\n",
    "        \"sorella\", \"sorelle\", \"casa\", \"case\", \"famiglia\", \"famiglie\", \"nonna\", \"nonne\", \"nonno\", \"nonni\",\n",
    "        \"genitore\", \"genitori\", \"parenti\", \"parente\"\n",
    "    ],\n",
    "    \"società\": [\n",
    "        \"gente\", \"popolo\", \"popoli\", \"strada\", \"strade\", \"mondo\", \"mondi\", \"libertà\", \"giustizia\", \"lavoro\",\n",
    "        \"lavori\", \"politica\", \"politiche\", \"diritto\", \"diritti\", \"realtà\", \"società\", \"comunità\", \"cultura\",\n",
    "        \"culture\", \"nazione\", \"nazioni\"\n",
    "    ],\n",
    "    \"religione\": [\n",
    "        \"dio\", \"dei\", \"fede\", \"preghiera\", \"preghiere\", \"paradiso\", \"paradisi\", \"inferno\", \"inferni\", \"anima\",\n",
    "        \"anime\", \"angelo\", \"angeli\", \"peccato\", \"peccati\", \"miracolo\", \"miracoli\", \"croce\", \"croci\", \"santo\",\n",
    "        \"santa\", \"santi\", \"sante\"\n",
    "    ],\n",
    "    \"musica\": [\n",
    "        \"musica\", \"musiche\", \"canzone\", \"canzoni\", \"nota\", \"note\", \"melodia\", \"melodie\", \"cantare\", \"cantato\",\n",
    "        \"cantata\", \"cantati\", \"cantate\", \"voce\", \"voci\", \"ritmo\", \"ritmi\", \"accordo\", \"accordi\", \"silenzio\",\n",
    "        \"silenzi\", \"suono\", \"suoni\"\n",
    "    ],\n",
    "    \"corpo\": [\n",
    "        \"mano\", \"mani\", \"occhio\", \"occhi\", \"viso\", \"visi\", \"pelle\", \"pelli\", \"labbro\", \"labbra\", \"respiro\",\n",
    "        \"respiri\", \"voce\", \"voci\", \"petto\", \"petti\", \"sguardo\", \"sguardi\", \"abbraccio\", \"abbracci\", \"corpo\",\n",
    "        \"corpi\", \"cuore\", \"cuori\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Parole positive\n",
    "positive_words = {\n",
    "    \"amore\", \"amori\", \"abbraccio\", \"abbracci\", \"allegria\", \"armonia\", \"aurora\", \"bellissimo\", \"bellissima\",\n",
    "    \"bene\", \"bontà\", \"brillante\", \"carezza\", \"carezze\", \"cielo\", \"conforto\", \"cuore\", \"dolcezza\", \"emozione\",\n",
    "    \"entusiasmo\", \"estate\", \"felice\", \"festa\", \"fiducia\", \"fortune\", \"gioia\", \"giusto\", \"incanto\", \"innamorato\",\n",
    "    \"intesa\", \"libertà\", \"luce\", \"meraviglia\", \"musica\", \"ottimismo\", \"pace\", \"paradiso\", \"passione\", \"positivo\",\n",
    "    \"promessa\", \"risata\", \"riso\", \"salvezza\", \"sereno\", \"sogno\", \"sorriso\", \"speranza\", \"splendore\", \"tenerezza\",\n",
    "    \"tranquillo\", \"trionfo\", \"vittoria\", \"vita\", \"gratitudine\", \"soddisfazione\", \"orgoglio\", \"successo\", \"felicità\",\n",
    "    \"euforia\", \"speranzoso\"\n",
    "}\n",
    "\n",
    "# Parole negative\n",
    "negative_words = {\n",
    "    \"odio\", \"tristezza\", \"solitudine\", \"disperazione\", \"lacrima\", \"pianto\", \"perdere\", \"perso\", \"addio\", \"vuoto\",\n",
    "    \"rimpianto\", \"assenza\", \"morte\", \"inferno\", \"peccato\", \"paura\", \"dolore\", \"bugia\", \"male\", \"malinconia\",\n",
    "    \"angoscia\", \"ansia\", \"invidia\", \"gelosia\", \"fallimento\", \"sconfitta\", \"disprezzo\", \"delusione\"\n",
    "}\n",
    "\n",
    "# Analisi testi\n",
    "data = []\n",
    "\n",
    "for year_folder in os.listdir(base_path):\n",
    "    year_path = os.path.join(base_path, year_folder)\n",
    "    if os.path.isdir(year_path):\n",
    "        for filename in os.listdir(year_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(year_path, filename)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    text = file.read()\n",
    "                    tokens = clean_and_tokenize(text)\n",
    "                    token_counts = Counter(tokens)\n",
    "\n",
    "                    sem_counts = {cat: sum(token_counts[word] for word in words) for cat, words in semantic_categories.items()}\n",
    "                    pos_count = sum(token_counts[word] for word in positive_words)\n",
    "                    neg_count = sum(token_counts[word] for word in negative_words)\n",
    "\n",
    "\n",
    "for year_folder in os.listdir(base_path):\n",
    "    year_path = os.path.join(base_path, year_folder)\n",
    "    if os.path.isdir(year_path):\n",
    "        # Estrai l'anno come numero da 'year_folder'\n",
    "        match = re.search(r\"\\d{4}\", year_folder)\n",
    "        year = int(match.group()) if match else year_folder  # Se non trova l'anno, usa il nome cartella originale\n",
    "\n",
    "        for filename in os.listdir(year_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(year_path, filename)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    text = file.read()\n",
    "                    tokens = clean_and_tokenize(text)\n",
    "                    token_counts = Counter(tokens)\n",
    "\n",
    "                    sem_counts = {\n",
    "                        cat: sum(token_counts[word] for word in words)\n",
    "                        for cat, words in semantic_categories.items()\n",
    "                    }\n",
    "\n",
    "                    pos_count = sum(token_counts[word] for word in positive_words)\n",
    "                    neg_count = sum(token_counts[word] for word in negative_words)\n",
    "\n",
    "                    data.append({\n",
    "                        \"anno\": year,\n",
    "                        \"brano\": filename.replace(\".txt\", \"\"),\n",
    "                        **sem_counts,\n",
    "                        \"positività\": pos_count,\n",
    "                        \"negatività\": neg_count\n",
    "                    })\n",
    "\n",
    "# Creazione DataFrame e salvataggio\n",
    "df = pd.DataFrame(data)\n",
    "df.sort_values(by=[\"anno\", \"brano\"], inplace=True)\n",
    "output_path = os.path.join(base_path, \"Analisi_Semantica_Sanremo.xlsx\")\n",
    "df.to_excel(output_path, index=False)\n",
    "print(f\"File salvato in: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec705d90-d008-4860-83be-9132022c6008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
